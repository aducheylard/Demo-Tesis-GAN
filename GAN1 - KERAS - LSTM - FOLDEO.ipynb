{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN1 - KERAS - LSTM - FOLDEO.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"1a4ec5485fc52906d8a7c8fcf8f39da33ca1f06d93010ba644db5c2b15d734d1"},"kernelspec":{"display_name":"Python 3.9.7 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Cpaf2o_ccI1U"},"source":["Comando para abrir el server desde la consaola:\n","\n","jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0 --no-browser --NotebookApp.token=abcd\n","\n","Luego copiar token de la consola al google collab para conectarse de forma local.\n","\n","Se le puede agregar el siguiente parametro para tener un token fijo.\n","\n","--NotebookApp.token=abcd"]},{"cell_type":"code","metadata":{"id":"nKGuuGAY3cau"},"source":["#!pip install lightkurve\n","#!pip install astropy\n","#!pip install pandas\n","#!pip install matplotlib\n","#!pip install --default-timeout=100 sklearn\n","#!pip install --default-timeout=100 tensorflow --user\n","#!pip install psutil\n","#!pip install tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfp2DshJ3WQR"},"source":["import astropy\n","from astropy import units as u\n","from astropy.time import Time\n","import numpy as np\n","from numpy import zeros, ones, vstack\n","from numpy.random import randn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","from sklearn.preprocessing import MinMaxScaler\n","from IPython import display\n","import time as ptime\n","from datetime import datetime\n","import lightkurve as lk\n","import statistics as st\n","import os, psutil\n","from os import listdir, path, mkdir\n","from os.path import isfile, join\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM\n","from keras.layers.embeddings import Embedding\n","import keras\n","from sklearn.model_selection import train_test_split\n","from configparser import ConfigParser"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWDS69mUwvFY"},"source":["#For Debugging. Probando formatos de tiempo.\n","#time = [2211.79713 + 2450000, 3280.78216, 4955.55749, 55377.148037, 55377.148037 - 2450000, 55377.148037 + 2450000]\n","#t = Time(time, format='mjd')\n","#t.ymdhms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voZLuZQ83WQV"},"source":["#PARAMETROS DE LOS MODELOS COMPARTIDOS\n","training = True #Si queremos entrenar los modelos (True) o cargarlos desde una carpeta (False)\n","cantidad_series_de_tiempo = 2000 #Cantidad de series de tiempo con las que vamos a trabajar del dataset.\n","cantidad_mediciones = 700 #Cantidad de mediciones que con la que vamos a entrenar el modelo.\n","path_datos = \".\\Archivos_profe\\\\DATOSOGLE\\\\LCs\" #Carpeta donde estan todas las series de tiempo\n","path_index = \".\\Archivos_profe\\\\DATOSOGLE\\\\OGLE_dataset.dat\" #Archivo donde se enlista la informacion de las series de tiempo, es como el index. ID CLASE PATH P_1 errorT1 N.\n","extension_series_de_tiempo = '.dat' #Extension de las series de tiempo proveniente de los dataset.\n","path_models_root = \".\\Archivos_profe\\modelos\" #Carpeta raiz donde guardaremos los modelos\n","path_models = \".\\Archivos_profe\\\\modelos\\\\GAIA-07_12_2021-12_43_11-RRAB\\\\modelos\\\\\" #Carpeta de donde cargamos los modelos. SOLO SI {training} = False\n","columnas_index = ['ID', 'Class', 'Path', 'N'] #Columnas que vamos a leer de las series de tiempo provenientes de los dataset.\n","nombre_clase = \"ED\" #Clase de estrella con la que vamos a trabajar.\n","\n","#PARAMETROS DEL MODELO GAN\n","epochs = 1000 #Cantidad de epochs para entrenar la GAN\n","batch_size = 10 #Tamaño del batch con la cual se entrenan los datos, una mitad son datos reales y la otra falsos.\n","sample_interval = 5 #Intervalo de Epochs para analizar el training de la GAN.\n","len_vec = 1 #Cantidad de atributos que se van a generar. (En este caso solo genero la magnitud).\n","n_input = None #Input de dimension que recibe el modelo (Dejo en 'None' para que sea generico)\n","\n","#PARAMETROS EXCLUSIVOS LSTM_SCORE\n","epochs_score = 200 #Cantidad de epochs para el entranamiento de la LSTM para el score de la GAN(aprox 400)\n","len_vec_lstm_score = 2 #Cantidad de features que vamos a analizar, en este caso es el tiempo y magnitud.\n","n_samples_evaluate = int(0.5 * cantidad_series_de_tiempo) #Cantidad de muestras generadas que se usaran para medir el score de la GAN mediante la LSTM. El otro 50% seran series reales."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTm46zNR3WQV"},"source":["def guarda_parametros(config, path_models_root, tiempo, lista_config):\n","    #print('len(lista_config):', len(lista_config))\n","    #print('lista_config:', lista_config)\n","    if 'OGLE' in lista_config[4]: \n","        location_archivo = path_models_root + '\\\\OGLE-' + tiempo + '\\\\'\n","    elif 'GAIA' in lista_config[4]:\n","        location_archivo = path_models_root + '\\\\GAIA-' + tiempo + '\\\\'\n","    elif 'WISE' in lista_config[4]:\n","        location_archivo = path_models_root + '\\\\WISE-' + tiempo + '\\\\'\n","    else:\n","        print('No se detectó un dataset en particular.')\n","\n","    #Tenemos las listas con los nombres de los parametros\n","    lista_nombre_variables_compartida = ['training', 'cantidad_series_de_tiempo', 'cantidad_mediciones', 'path_datos',\n","                                         'path_index', 'extension_series_de_tiempo', 'path_models_root', 'path_models', 'columnas_index', 'nombre_clase']\n","    lista_nombre_variables_GAN = ['epochs', 'batch_size', 'sample_interval', 'len_vec', 'n_input']\n","    lista_nombre_variables_LSTM_Score = ['epochs_score', 'len_vec_lstm_score', 'n_samples_evaluate']\n","\n","    #Creamos las secciones del archivo.\n","    if not config.has_section('Parametros compartidos'):\n","        config.add_section('Parametros compartidos')\n","    if not config.has_section('GAN'):\n","        config.add_section('GAN')\n","    if not config.has_section('LSTM_score'):\n","        config.add_section('LSTM_score')\n","\n","    #Creamos la carpeta y el archivo si no existe.\n","    if not path.exists(path_models_root):\n","        mkdir(path_models_root)\n","    if not path.exists(location_archivo):\n","        mkdir(location_archivo)\n","\n","    #Asiganmos las variables a las distintas secciones:\n","    #Parametros inciales\n","    for ite in range(len(lista_nombre_variables_compartida)): #0-11\n","        config.set('Parametros compartidos', lista_nombre_variables_compartida[0], str(lista_config[0]))\n","        lista_config.remove(lista_config[0])\n","        lista_nombre_variables_compartida.remove(lista_nombre_variables_compartida[0])\n","    for ite in range(len(lista_nombre_variables_GAN)):\n","        config.set('GAN', lista_nombre_variables_GAN[0], str(lista_config[0]))\n","        lista_config.remove(lista_config[0])\n","        lista_nombre_variables_GAN.remove(lista_nombre_variables_GAN[0])\n","    for ite in range(len(lista_nombre_variables_LSTM_Score)):\n","        config.set('LSTM_score', lista_nombre_variables_LSTM_Score[0], str(lista_config[0]))\n","        lista_config.remove(lista_config[0])\n","        lista_nombre_variables_LSTM_Score.remove(lista_nombre_variables_LSTM_Score[0])\n","    #Como luego queremos guardar los resultados, estos parametros iniciales no los vamos a guardar al archivo. Sino, que cuando reciba los resultados, se guardara todo.\n","\n","def guarda_resultados(config, path_models_root, tiempo, lista_resultados):\n","    if 'OGLE' in path_index: \n","        location_archivo = path_models_root + '\\\\OGLE-' + tiempo + '\\\\'\n","    elif 'GAIA' in path_index:\n","        location_archivo = path_models_root + '\\\\GAIA-' + tiempo + '\\\\'\n","    elif 'WISE' in path_index:\n","        location_archivo = path_models_root + '\\\\WISE-' + tiempo + '\\\\'\n","    else:\n","        print('No se detectó un dataset en particular.')\n","    #location_archivo = path_models_root + '\\\\' + tiempo + '\\\\'\n","    nombre_archivo = \"parametros.init\"\n","    final_path = location_archivo + nombre_archivo\n","\n","    #Creamos las secciones del archivo.\n","    if not config.has_section('Resultados'):\n","        config.add_section('Resultados')\n","    #config.add_section('Resultados')\n","\n","    lista_nombre_resultados = ['accuracy_LSTM_score_GAN', 'periodo_generado', 'periodo_real']\n","\n","    #Resultados\n","    print(lista_resultados)\n","    for ite in range(len(lista_nombre_resultados)):\n","        config.set('Resultados', lista_nombre_resultados[0], str(lista_resultados[0]))\n","        lista_resultados.remove(lista_resultados[0])\n","        lista_nombre_resultados.remove(lista_nombre_resultados[0])\n","\n","    #Guardamos las secciones al archivo.\n","    with open(final_path, 'a+') as f:\n","        config.write(f)\n","\n","    print(\"Parametros guardados en:\", final_path)\n","\n","def guarda_modelo(ruta_instancia, modelo, nombre):\n","    extension = \".h5\"\n","    ruta_instancia = ruta_instancia + '\\\\modelos'\n","    path_models = ruta_instancia + '\\\\' + str(nombre) + str(extension)\n","    #Creamos la carpeta si no existe.\n","    if not path.exists(ruta_instancia):\n","        mkdir(ruta_instancia)\n","    print(path_models)\n","    modelo.save(path_models)\n","\n","def obtiene_clases(df_index_original):\n","    #Mostramos todas las clases que tiene el dataset\n","    lista_clases = df_index_original['Class'].unique().tolist()\n","    #lista_clases = ['ED', 'ESD', 'EC', 'cep', 'dsct', 'RRab', 'RRc', 'std', 'OSARG', 'SRV', 'Mira']\n","    print(\"Lista de clases disponibles del index:\", lista_clases)\n","    return lista_clases\n","\n","def lista_cantidad_series_de_tiempo_por_clase(df_index, lista_clases):\n","    #Este ciclo y variables son solo para imprimir su detalle.\n","    for clase in lista_clases:\n","        df_index_clase = df_index\n","        #Filtramos el df solo con la clase que estamos analizando.\n","        df_index_clase = df_index.loc[df_index['Class'] == clase]\n","        #Filtramos el df dejando {cantidad_mediciones} mediciones o más. Si queremos mostrar la cantidad total de sires de tiempo por clase hay que definir {cantidad_mediciones} = 1\n","        df_index_clase = df_index_clase.loc[df_index_clase['N'] >= 1]\n","        print(\"La clase \\\"\" + clase + '\\\" tiene: ' + str(len(df_index_clase)) + \" series de tiempo.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lUSej8fwvFb"},"source":["#LISTA DONDE ALMACENAMOS LOS PARAMETROS DE LOS MODELOS PARA GUARDAR EN UN .init.\n","lista_config = []\n","lista_config.extend((training, cantidad_series_de_tiempo, cantidad_mediciones, path_datos, path_index, extension_series_de_tiempo, path_models_root, path_models\n","                     , columnas_index, nombre_clase, epochs, batch_size, sample_interval, len_vec, n_input, epochs_score, len_vec_lstm_score, n_samples_evaluate))\n","\n","#Obtenemos la variable tiempo para guardar los modelos.\n","tiempo = datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\") + '-' + str(nombre_clase)\n","\n","if 'OGLE' in path_index: \n","    ruta_instancia = path_models_root + '\\\\OGLE-' + str(tiempo)\n","elif 'GAIA' in path_index:\n","    ruta_instancia = path_models_root + '\\\\GAIA-' + str(tiempo)\n","elif 'WISE' in path_index:\n","    ruta_instancia = path_models_root + '\\\\WISE-' + str(tiempo)\n","else:\n","    print('No se detectó un dataset en particular.')\n","    ruta_instancia = path_models_root + '\\\\' + str(tiempo)\n","#Guardamos los parametros iniciales.\n","config = ConfigParser()\n","guarda_parametros(config, path_models_root, tiempo, lista_config)\n","\n","#Creamos una lista donde almacenaremos ciertos resultados para guardar a un archivo.\n","lista_resultados = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPuQ7lNt3WQW"},"source":["#Leemos el archivo de indice o de info de las series de tiempo.\n","df_index_original = pd.read_csv(path_index, sep=',', skiprows=1, names=columnas_index)\n","#Eliminamos las columnas que no nos interesan.\n","df_index_filtrada = df_index_original.drop(columns=['Path'])\n","#Obtenemos la lista de clases que existen en el archivo index.\n","lista_clases = obtiene_clases(df_index_filtrada)\n","\n","#Se muestra la cantidad total de mediciones que tiene cada serie de tiempo originalmente.\n","lista_cantidad_series_de_tiempo_por_clase(df_index_filtrada, lista_clases)\n","#Filtramos el df solo con la clase que estamos analizando.\n","df_index_filtrada = df_index_filtrada.loc[df_index_filtrada['Class'] == nombre_clase]\n","#Filtramos el df dejando {cantidad_mediciones} mediciones o más.\n","df_index_filtrada = df_index_filtrada.loc[df_index_filtrada['N'] >= cantidad_mediciones]\n","if df_index_filtrada.empty:\n","    print(\"\\n##### ERROR DF ESTA VACIO. SE DEBE DISMINUIR LA CANTIDAD DE MEDICIONES #####\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMWuF8B_5ViB"},"source":["#Pasamos la lista COMPLETA (incluye todas las clases) a un df.\n","df_listdir = pd.DataFrame(listdir(path_datos), columns =['ID'])\n","\n","#Para asegurar una union correcta, convertimos las columnas de 'ID' a str. Tambien se puede pasar a int o int64 pero arroja errores con ciertos numeros en particular.\n","df_listdir['ID'] = df_listdir['ID'].astype(str)\n","df_index_filtrada['ID'] = df_index_filtrada['ID'].astype(str)\n","#Realizamos una union entre los df para descartar las series de tiempo que no pertenezcan a la clase.\n","df_union = df_index_filtrada.join(df_listdir.set_index('ID'), on='ID')\n","#Si el .join no funciona (para el caso de WISE) se utiliza otro metodo.\n","if df_union.empty:\n","    df_union = pd.merge(df_index_filtrada, df_listdir, on='ID')\n","\n","#Convertimos el df a int.\n","#print(df_union) #Tiene los nombres y la cantidad de mediciones (ya filtradas) de las series de tiempo de la clase en cuestion."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8NteegB3WQW"},"source":["#CARGAMOS DATOS\n","lista_instancias = []\n","lista_nombre_instancias = []\n","for index, row in df_union.iterrows():\n","    nombre = row['ID']\n","    ruta = join(path_datos, nombre + extension_series_de_tiempo)\n","    #print(ruta)\n","    #Cargamos el archivo a un df para su procesamiento. Podemos agregar la cantidad de columnas que queramos en caso de que venga con más.\n","    #Recordar cambiar el tipo de separacion en funcion de los dataset.\n","    if 'OGLE' in path_index: \n","        df = pd.read_csv(ruta, sep=' ', names=['tiempo', 'magnitud', 'error_magnitud', '1', '2', '3', '4'])\n","    elif 'GAIA' in path_index:\n","        df = pd.read_csv(ruta, sep=',', names=['tiempo', 'magnitud', 'error_magnitud', '1', '2', '3', '4'], skiprows=1)\n","    elif 'WISE' in path_index:\n","        df = pd.read_csv(ruta, sep=',', names=['tiempo', 'magnitud', 'error_magnitud', '1', '2', '3', '4'], skiprows=1)\n","    else:\n","        print(\"ERROR #### NO SE PUDO CARGAR LA SERIE DE TIEMPO ####\")\n","    if len(df.columns) >= 3:\n","        #Borramos las columnas innecesarias.\n","        while len(df.columns) >= 3:\n","            df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n","    else:\n","        print(\"ERROR #### LOS DATOS NO TIENEN 2 O 3 COLUMNAS ####\")\n","        continue\n","    \n","    #Agregamos el nombre de la instancia a una lista.\n","    lista_nombre_instancias.append(nombre)\n","    #Cortamos el df para tener solo la cantidad de mediciones definidas.\n","    df = df[:cantidad_mediciones]\n","    df = df.dropna()\n","    #Convertimos el df a un array.\n","    array = df.to_numpy()\n","    #Agregamos el array a una lista.\n","    lista_instancias.append(array)\n","\n","\n","#Convertimos la lista de las series de tiempo a un ndarray.\n","samples = np.array(lista_instancias, dtype=object)\n","print(type(samples)) #<class 'numpy.ndarray'>\n","print(samples.shape) #(5551, 700, 2)...........Si no tiene este formato de 3 dimensiones, hay un error."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zD0ft_r3WQX"},"source":["#Seleccionamos la cantidad de series de tiempo a trabajar (puede ser una muestra o utilizar todos los datos si es que son compatibles con los modelos)\n","samples = samples[0:cantidad_series_de_tiempo,:,:]\n","print(samples.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMPisNaciDmB"},"source":["#PROCESAMOS LA VARIABLES {sample}\n","#TRABAJAMOS CON LA MAGNITUD\n","\n","#Se elimina el tiempo en las mediciones\n","samples_magnitud_sin_norm = np.delete(samples, (0), axis=2)\n","#Cambiamos la dimension\n","samples_magnitud_sin_norm_2dim = samples_magnitud_sin_norm.reshape([cantidad_series_de_tiempo, cantidad_mediciones])\n","#Normalizamos los datos entre 0 y 1\n","scaler_mag_normal = MinMaxScaler(feature_range=(0, 1))\n","scaler_mag_normal = scaler_mag_normal.fit(samples_magnitud_sin_norm_2dim.T)\n","samples_magnitud_norm_2dim = scaler_mag_normal.transform(samples_magnitud_sin_norm_2dim.T).T\n","#Cambiamos la dimension\n","samples_magnitud_norm_3dim = np.reshape(samples_magnitud_norm_2dim, (cantidad_series_de_tiempo, cantidad_mediciones, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kocjaju3WQY"},"source":["#PROCESAMOS LA VARIABLES {sample}\n","#TRABAJAMOS CON EL TIEMPO\n","\n","#Se elimina la magnitud en las mediciones\n","samples_tiempo_sin_norm = np.delete(samples, (1), axis=2)\n","#Cambiamos la dimension\n","samples_tiempo_sin_norm_2dim = samples_tiempo_sin_norm[:,:,0].reshape([cantidad_series_de_tiempo,cantidad_mediciones])\n","\n","#Calculamos la diferencia del tiempo sin previa normalizacion\n","diff_samples_tiempo_sin_norm_2dim_temporal = np.diff(samples_tiempo_sin_norm_2dim)\n","diff_samples_tiempo_sin_norm_2dim = np.zeros_like(samples_tiempo_sin_norm_2dim)\n","diff_samples_tiempo_sin_norm_2dim[:,1:] = diff_samples_tiempo_sin_norm_2dim_temporal\n","    \n","#Normalizamos el tiempo ya diferenciado\n","scaler_tiempo_normal = MinMaxScaler(feature_range=(0, 1))\n","scaler_tiempo_normal = scaler_tiempo_normal.fit(diff_samples_tiempo_sin_norm_2dim.T)\n","diff_samples_tiempo_norm_2dim = scaler_tiempo_normal.transform(diff_samples_tiempo_sin_norm_2dim.T).T\n","\n","#Cambiamos la dimension\n","diff_samples_tiempo_norm_3dim = np.reshape(diff_samples_tiempo_norm_2dim,(cantidad_series_de_tiempo,cantidad_mediciones,1))\n","\n","#Para el training del modelo LSTM para conocer el score de la GAN.\n","sum_acum_diff_tiempo_norm_3dim = np.cumsum(diff_samples_tiempo_norm_3dim, axis=1)\n","time_series_original_procesada = np.vstack((sum_acum_diff_tiempo_norm_3dim.T, samples_magnitud_norm_3dim.T)).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3LyoC7k3WQY"},"source":["#REALIZAMOS BOOTSTRAPING DE TIEMPO\n","\n","# Instanciamos la lista temporal que tendra el bootstraping de samples\n","bootstraped = [] \n","\n","#Realizamos el bootstrapping sobre la variable samples\n","for i in range(cantidad_series_de_tiempo):\n","    random.seed = datetime.today()\n","    x = random.randint(0, len(samples)-1)\n","    bootstraped.append(samples[x])\n","    \n","#Convertimos la lista temporal a un ndarray\n","samples_boot = np.array(bootstraped, ndmin=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDRoHZ343wT9","scrolled":true},"source":["#REALIZAMOS EL PROCESAMIENTO DE DATOS DEL TIEMPO BOOTSTRAPEADO\n","\n","#Se elimina la magnitud en las mediciones bootstrapeadas\n","samplesBoot_tiempo_sin_norm_3dim = np.delete(samples_boot, (1), axis=2)\n","\n","#Modificamos la dimension para calcular las diferencias de tiempo\n","samplesBoot_tiempo_sin_norm_2dim = samplesBoot_tiempo_sin_norm_3dim[:,:,0].reshape([cantidad_series_de_tiempo, cantidad_mediciones])\n","\n","#Calculamos la diferencia del tiempo bootstrapeado sin previa normalizacion\n","diff_samplesBoot_tiempo_sin_norm_2dim_temporal = np.diff(samplesBoot_tiempo_sin_norm_2dim)\n","diff_samplesBoot_tiempo_sin_norm_2dim = np.zeros_like(samplesBoot_tiempo_sin_norm_2dim)\n","diff_samplesBoot_tiempo_sin_norm_2dim[:,1:] = diff_samplesBoot_tiempo_sin_norm_2dim_temporal\n","\n","#Normalizamos el tiempo bootstrapeado ya diferenciado\n","scaler_boot = MinMaxScaler(feature_range=(0, 1))\n","scaler_boot = scaler_boot.fit(diff_samplesBoot_tiempo_sin_norm_2dim.T)\n","diff_samplesBoot_tiempo_norm_2dim = scaler_boot.transform(diff_samplesBoot_tiempo_sin_norm_2dim.T).T\n","\n","#Tiempo bootstrapeado final\n","diff_samplesBoot_tiempo_norm_3dim = np.reshape(diff_samplesBoot_tiempo_norm_2dim,(cantidad_series_de_tiempo,cantidad_mediciones,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cytaebww3WQZ"},"source":["#MODELOS GAN\n","\n","def define_discriminator(n_input, cantidad_mediciones):\n","    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    model = Sequential(name='Discriminador')\n","    model.add(LSTM(128, activation='tanh', batch_size=n_input, input_shape=(cantidad_mediciones, 1), return_sequences=True))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def define_generator(n_input, cantidad_mediciones):\n","    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    model = Sequential(name='Generador')\n","    model.add(LSTM(128, activation='tanh', batch_size=n_input, input_shape=(cantidad_mediciones, 2), return_sequences=True))\n","    #Para los experimentos de dos capas usar la siguiente capa oculta.\n","    #model.add(LSTM(64, activation='tanh', return_sequences=True))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def define_gan(g_model, d_model):\n","    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    #No entrenamos los pesos del discriminador\n","    d_model.trainable = False\n","    #Por bug al guardar los modelos the keras, debemos sobreescribir que si queremos entrenar los pesos del generador.\n","    g_model.trainable = True\n","    model = Sequential(name='GAN')\n","    model.add(g_model)\n","    model.add(d_model)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    model.summary()\n","    return model\n","\n","#MODELO LSTM PARA EL SCORE DE LA GAN\n","def define_gan_score(cantidad_mediciones, len_vec_lstm_score):\n","    lstm_score_model = Sequential(name='LSTM_Score')\n","    lstm_score_model.add(LSTM(128, activation='tanh', input_shape=(cantidad_mediciones, len_vec_lstm_score), return_sequences=True))\n","    lstm_score_model.add(Dense(2, activation='sigmoid'))\n","    lstm_score_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    lstm_score_model.summary()\n","    return lstm_score_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LarGECIb3WQZ"},"source":["#INSTANCIAMOS LOS MODELOS\n","if training:\n","    d_model = define_discriminator(n_input, cantidad_mediciones)\n","    g_model = define_generator(n_input, cantidad_mediciones)\n","    gan_model = define_gan(g_model, d_model)\n","    lstm_score_model = define_gan_score(cantidad_mediciones, len_vec_lstm_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um3ck_Q-3WQa"},"source":["#Funcion personal para plotear datos\n","def plot_timeseries(tiempo, magnitud):\n","    # TIEMPO TIENE QUE VENIR DIFERENCIADO, NORMALIZADO\n","    # MAGNITUD TIENE QUE VENIR NORMALIZADA\n","    tiempos = tiempo[:,:,0]\n","    tiempo = np.cumsum(tiempos, axis=1)\n","    plt.plot(tiempo[0,:], magnitud[0,:], 'o', markersize=1)\n","    plt.xlabel('Tiempo DIF-NORM-ACUM')\n","    plt.ylabel('Magnitud NORM')\n","    plt.title('Serie de tiempo generada')\n","    plt.show()\n","\n","#Funcion original de ploteo de Alvaro, plotea la serie real. Serie de tiempo 0, 2, 4 y 6\n","def plot_series_real(samples_magnitud_norm_3dim, diff_samples_tiempo_norm_3dim, ruta_instancia):    \n","    mag = samples_magnitud_norm_3dim[:,:,0] #magnitud\n","    tiempo = diff_samples_tiempo_norm_3dim[:,:,0] #tiempo\n","    suma_acumulada_tiempo = np.cumsum(tiempo, axis=1)\n","    fig = plt.figure(figsize=(45, 10))\n","    plt.axis('off')\n","    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n","    for i in range(0, 4):\n","        ax = fig.add_subplot(2, 2 , i+1)\n","        ax.title.set_text('Serie de tiempo real - %s' % (2*i))\n","        #Multiplico por 2 para no mirar series de tiempo tan juntas\n","        ejeX = suma_acumulada_tiempo[2*i,:] #[0,:] [2,:] [4,:] [6,:]\n","        ejeY = mag[2*i,:] #[0,:] [2,:] [4,:] [6,:]\n","        ax.scatter(ejeX, ejeY, color='black', marker='o')\n","    nombre_guardado = '%s\\\\%s' % (ruta_instancia, 'Plot-real.png')\n","    plt.savefig(nombre_guardado)\n","    plt.show()\n","\n","#Funcion original de ploteo de Alvaro, plotea la serie de tiempo 0, 2, 4 y 6 generada mientras se va entrenando\n","def plot_series_gen(fake_generated_data, diff_samplesBoot_tiempo_norm_3dim, batch_size, cantidad_mediciones, epoch, ruta_instancia):\n","    #print(\"Ploteando serie generada\")\n","    mag_generated_data = fake_generated_data[:10,:,0].reshape([batch_size,cantidad_mediciones])\n","    tiempo_samples = diff_samplesBoot_tiempo_norm_3dim[:,:,0]\n","    sum_acum_tiempo_samples = np.cumsum(tiempo_samples, axis=1)\n","    fig = plt.figure(figsize=(45, 10))\n","    plt.axis('off')\n","    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n","    for i in range(0, 4):\n","        ax = fig.add_subplot(2, 2 , i+1)\n","        ax.title.set_text('Magnitud generada - %s - %s' % (epoch, 2*i))\n","        ax.set_xlabel('Tiempo DIF-NORM-ACUM')\n","        ax.set_ylabel('Magnitud normalizada')\n","        #Multiplico por 2 para no mirar series de tiempo tan juntas\n","        ejeX = sum_acum_tiempo_samples[2*i,:] # [0,:] [2,:] [4,:] [6,:]\n","        ejeY = mag_generated_data[2*i,:] # [0,:] [2,:] [4,:] [6,:]\n","        ax.scatter(ejeX, ejeY, color='black', marker='o')\n","    ruta_guardado = ruta_instancia + '\\generados\\datos'\n","    if not path.exists(ruta_instancia):\n","        mkdir(ruta_instancia)\n","        mkdir(ruta_instancia + '\\generados')\n","        mkdir(ruta_guardado)\n","    else:\n","        if not path.exists(ruta_instancia + '\\generados'):\n","            mkdir(ruta_instancia + '\\generados')\n","            mkdir(ruta_guardado)\n","        elif not path.exists(ruta_guardado):\n","            mkdir(ruta_guardado)\n","    nombre_guardado = '%s\\\\%s-%s%s' % (ruta_guardado, 'Plot-generado', epoch, '.png')\n","    #print(nombre_guardado)\n","    plt.savefig(nombre_guardado)\n","    plt.show()\n","    \n","\n","def sample_Z(batch_size, cantidad_mediciones, len_vec):\n","    sample = np.float32(np.random.normal(size=[batch_size, cantidad_mediciones, len_vec]))\n","    return sample\n","\n","def get_batch(samples, batch_size, batch_idx):\n","    #print(\"samples\", samples.shape)\n","    #print(\"batch_size\", batch_size)\n","    #print(\"batch_idx\", batch_idx)\n","    start_pos = batch_size * batch_idx\n","    end_pos = start_pos + batch_size\n","    batch = samples[start_pos:end_pos,:,:]\n","    y = np.ones(batch.shape)\n","    return batch, y\n","\n","def generate_latent_points(n_samples, cantidad_mediciones, len_vec):\n","    #Generamos un punto en el espacio latente\n","    x_input = randn(int(n_samples * cantidad_mediciones * len_vec))\n","    #Redimensionamos con el tamano de inputs de batches\n","    x_input = x_input.reshape(n_samples, cantidad_mediciones, len_vec)\n","    return x_input\n","\n","def generate_fake_samples(datos, g_model, batch_size, cantidad_mediciones, len_vec, batch_idx):\n","    #Generamos un punto en el espacio latente\n","    x_input_random = generate_latent_points(batch_size, cantidad_mediciones, len_vec) #Random generado\n","    #Buscamos datos reales\n","    x_input_data, _= get_batch(datos, batch_size, batch_idx)\n","    #Juntamos magnitudes reales y falsas\n","    z = np.vstack((x_input_data.T, x_input_random.T)).T\n","    #Generamos datos\n","    X = g_model.predict(z)\n","    y = zeros(X.shape)\n","    return X, y\n","\n","def train_generator(time_series_original_procesada, cantidad_mediciones, batch_size, batch_idx, g_rounds, offset, cantidad_series_de_tiempo):\n","    tiempo_3dim = np.reshape(time_series_original_procesada[:,:,0], (cantidad_series_de_tiempo,cantidad_mediciones,1))\n","    # update the generator\n","    for g in range(g_rounds):\n","        #Sacamos un batch o muestra de tiempo\n","        real_time_batch, _ = get_batch(tiempo_3dim, batch_size, batch_idx + g + offset) \n","        #Obtenemos una muestra de puntos latentes\n","        sample_latent_point = sample_Z(batch_size, cantidad_mediciones, 1)\n","        data_for_training_gan = np.vstack((real_time_batch.T, sample_latent_point.T)).T\n","        #print(data_for_training_gan)\n","        #Generamos los labels de 1's\n","        y_gan = np.ones((batch_size, cantidad_mediciones, 1))\n","        #Entrenamos el generador con los datos creados sin cambiar el peso del discriminador\n","        g_loss = gan_model.train_on_batch(data_for_training_gan, y_gan)\n","    return g_loss\n","\n","def train_discriminator(time_series_original_procesada, batch_size, batch_idx, d_rounds, offset, cantidad_series_de_tiempo, cantidad_mediciones):\n","    # update the discriminator\n","    magnitud_3dim = np.reshape(time_series_original_procesada[:,:,1], (cantidad_series_de_tiempo,cantidad_mediciones,1))\n","    #tiempo_3dim = np.reshape(time_series_original_procesada[:,:,0], (cantidad_series_de_tiempo,cantidad_mediciones,1))\n","    for d in range(d_rounds):\n","        #Obtenemos datos reales para el posterior training\n","        X_real_magnitud, y_real = get_batch(magnitud_3dim, batch_size, batch_idx + d + offset)\n","        #Entrenamos con los datos REALES\n","        d_loss_real, d_accu_real = d_model.train_on_batch(X_real_magnitud, y_real) #{'loss': x.xxxxx, 'accuracy': x.xxxxx} #ENTRENAMOS CON DATOS REALES\n","        #Obtenemos magnitudes generadas condicionadas con magnitudes reales\n","        X_fake, y_fake = generate_fake_samples(magnitud_3dim, g_model, batch_size, cantidad_mediciones, len_vec, batch_idx + d + offset)\n","        #Entrenamos con magnitudes generadas y condicionadas con la magnitud real\n","        d_loss_fake, d_accu_fake = d_model.train_on_batch(X_fake, y_fake, return_dict=False) #{'loss': x.xxxxx, 'accuracy': x.xxxxx}\n","        #Sumamos la perdida de los entrenamientos\n","        disc_loss_total = d_loss_real + d_loss_fake\n","    return disc_loss_total\n","\n","def foldeo_training(cantidad_series_de_tiempo, cantidad_mediciones, samplesBoot_tiempo_sin_norm_3dim, mag_fake_generated_data, scaler_mag_normal, ruta_instancia, epoch):\n","    #Nombre de columnas de la instnacia del archivo de OGLE\n","    nombre_columnas = ['time', 'magnitud', 'error'] \n","    \n","    #Procesamos los datos generados para foldearlos.\n","    generated_mag_2dim = mag_fake_generated_data.reshape(cantidad_series_de_tiempo, cantidad_mediciones)\n","    #Desnormalizamos la magnitud generada.\n","    generated_mag_sin_norm_2dim = scaler_mag_normal.inverse_transform(generated_mag_2dim.T).T\n","    #Cambiamos la dimension.\n","    generated_mag_sin_norm_3dim = generated_mag_sin_norm_2dim.reshape((cantidad_series_de_tiempo, cantidad_mediciones, 1))\n","    #Juntamos el tiempo boostrapeado con la magnitud generada.\n","    time_series_generada_3dim = np.vstack((samplesBoot_tiempo_sin_norm_3dim.T, generated_mag_sin_norm_3dim.T)).T\n","\n","    #Rellenamos la columna 'error' de la libreria para foldear (no podemos foldear sin esta columna).\n","    test_0 = np.zeros((cantidad_series_de_tiempo, cantidad_mediciones, 1))\n","    #Concatenamos la columna de 0's.\n","    time_series_generada_3dim = np.vstack((time_series_generada_3dim.T, test_0.T)).T\n","\n","    ruta_datos = ruta_instancia + '\\generados\\datos'\n","    ruta_periodogramas = ruta_instancia + '\\generados\\periodogramas'\n","    ruta_foldeos = ruta_instancia + '\\generados\\\\foldeos'\n","\n","    if not path.exists(ruta_instancia + '\\generados'):\n","        mkdir(ruta_instancia + '\\generados')\n","        mkdir(ruta_datos)\n","        mkdir(ruta_periodogramas)\n","        mkdir(ruta_foldeos)\n","    if not path.exists(ruta_datos):\n","        mkdir(ruta_datos)\n","    if not path.exists(ruta_periodogramas):\n","        mkdir(ruta_periodogramas)\n","    if not path.exists(ruta_foldeos):\n","        mkdir(ruta_foldeos)\n","\n","    #Como queremos foldear los mismos datos que ploteamos cuando imprimos los datos en el training, necesitamos foldear 4 veces, ya que mostramos 4 plots.\n","    for i in range(0, 4):\n","        #Pasamos la serie de tiempo al df y utilizamos solo 1 serie de tiempo para ver el foldeo.\n","        df_generado = pd.DataFrame(time_series_generada_3dim[2*i,:,:], columns=nombre_columnas)\n","        #Guardamos los datos generados de esta evaluacion\n","        ruta = '%s\\\\%s-%s-%s%s' % (ruta_datos, 'SerieTiempo-Generada', epoch, 2*i, '.csv')\n","        df_generado.to_csv(path_or_buf=ruta, sep=' ', index=False)\n","        #Sumamos un escalar al tiempo para un foldeo correcto. Sacado de la pagina del OGLE.\n","        #PARA WISE NO HAY QUE SUMARLE LOS 2450000 YA QUE VIENEN EN FORMATO MJD\n","        if \"WISE\" in ruta_instancia:\n","            df_generado['time'] = df_generado['time']\n","        else:\n","            df_generado['time'] = df_generado['time'] + 2450000\n","\n","        df_generado_dtype_float = df_generado.astype(float) #Creamos una copia del {df_generado} como flotante.\n","        #df_generado_dtype_float = df_generado\n","\n","        times_no_format = df_generado_dtype_float['time']\n","        if \"WISE\" in ruta_instancia:\n","            times_jd = Time(times_no_format, format='mjd', scale='utc') #Si el formato es ambiguo, puede no pasarse. La escala tambien es opcional.\n","        else:\n","            times_jd = Time(times_no_format, format='jd', scale='utc') #Si el formato es ambiguo, puede no pasarse. La escala tambien es opcional.\n","        #df_generado_dtype_float['time'] = times_jd\n","        #print(df_generado_dtype_float['time'])\n","\n","        #list(Time.FORMATS)\n","        #Convertimos el df a un LightCurve\n","        #lc_generado = lk.LightCurve(time=df_generado_dtype_float['time'], flux=df_generado_dtype_float['magnitud'], flux_err=df_generado_dtype_float['error'])\n","        lc_generado = lk.LightCurve(time=times_jd, flux=df_generado_dtype_float['magnitud'], flux_err=df_generado_dtype_float['error'])\n","        #lc_generado_flatten = lc_generado.remove_nans()\n","        #lc_generado.plot(title=\"Scatter lightCruve generado - \" + str(epoch) + ' - ' + str(2*i))\n","        #plt.pause(0.0001)\n","\n","        #Calculamos el periodograma usando el metodo de'lombscargle'\n","        pg_generado = lc_generado.to_periodogram(method='lombscargle', oversample_factor=10)\n","        #pg_generado.plot()\n","        #plt.pause(0.0001)\n","\n","        #INFO\n","        #pg_generado.show_properties()\n","        #print(\"Frequency at max power:\", pg_generado.frequency_at_max_power)\n","        #print(\"Periodo:\", pg_generado.period_at_max_power)\n","\n","        #PLOTEAMOS EL PERIODOGRAMA\n","        plt.rcParams['figure.figsize'] = [3, 3]\n","        periodograma = pg_generado.plot(title=\"Periodograma generado - \" + str(epoch) + ' - ' + str(2*i))\n","        plt.pause(0.0001)\n","        ruta = '%s\\\\%s-%s-%s%s' % (ruta_periodogramas, 'Periodograma-generado', epoch, 2*i,'.png')\n","        periodograma.figure.savefig(ruta)\n","        plt.rcParams['figure.figsize'] = [10, 7]\n","        \n","        #FOLDEO\n","        #No puedo foldear con la fase normalizada en la misma funcion. NO HE PODIDO SOLUCIONARLO.\n","        lc_generado_foldeado = lc_generado.fold(pg_generado.period_at_max_power, normalize_phase=False)\n","        lc_scatter = lc_generado_foldeado.scatter(color='red')\n","        lc_scatter.set_title(\"Scatter - Serie de tiempo generada - \" + str(epoch) + ' - ' + str(2*i))\n","        lc_scatter.plot()\n","        plt.pause(0.0001)\n","        ruta = '%s\\\\%s-%s-%s%s' % (ruta_foldeos, 'Scatter-SerieTiempo-Generada', epoch, 2*i,'.png')\n","        lc_scatter.figure.savefig(ruta)\n","\n","        #Normalizamos las magnitudes\n","        #lc_generado_normalizado = lc_generado.normalize()\n","\n","        #Intento de samplear y normalizar el foldeo\n","        #median_generada = st.median(lc_generado_foldeado['flux'])\n","        mean_generada = st.mean(lc_generado_foldeado['flux'].value)\n","        #stddev_generada = st.stdev(lc_generado_foldeado['flux'].value)\n","\n","        lc_generado_foldeado['flux_norm'] = lc_generado_foldeado['flux'] / mean_generada\n","\n","        lc_generado_foldeado_binned = lc_generado_foldeado.bin(0.08)\n","\n","        #print(\"FOLDEADO\")\n","        #print(lc_generado_foldeado)\n","        #print(\"BINEADO\")\n","        #print(lc_generado_foldeado_binned)\n","        #plt.clf() #Vacia los datos del plot pero mantiene los ejes.\n","        plt.plot(lc_generado_foldeado.time.jd, lc_generado_foldeado['flux_norm'], 'k.')\n","        plt.plot(lc_generado_foldeado_binned.time_bin_start.jd, lc_generado_foldeado_binned['flux_norm'], 'r-', drawstyle='steps-mid')\n","        plt.xlabel('Time (days)')\n","        plt.ylabel('Normalized flux')\n","        plt.title(\"Foldeo Generado - \" + str(epoch) + ' - ' + str(2*i))\n","        ruta = '%s\\\\%s-%s-%s%s' % (ruta_foldeos, 'Foldeo-generado', epoch, 2*i,'.png')\n","        plt.savefig(ruta)\n","        plt.show()\n","        plt.pause(0.0001)\n","        #plt.close()\n","\n","#Funcion principal del training\n","def train(time_series_original_procesada, diff_samples_tiempo_norm_3dim, g_model, gan_model, epochs, batch_size, sample_interval, cantidad_mediciones, cantidad_series_de_tiempo, ruta_instancia, scaler_mag_normal):\n","    total_time = 0\n","    d_loss_total = []\n","    d_loss_total_per_epoch = []\n","    g_loss_total = []\n","    g_loss_total_per_epoch = []\n","    #Cantidad de veces que entrenamos cada modelo por iteracion\n","    d_rounds = 1\n","    g_rounds = 3\n","\n","    for epoch in range(epochs):\n","        #Iniciamos timer del epoch\n","        start = ptime.time()\n","        for batch_idx in range(0, int(len(time_series_original_procesada) / batch_size) - (d_rounds + g_rounds), d_rounds + g_rounds):\n","            if epoch % 2 == 0:\n","                g_loss = train_generator(time_series_original_procesada, cantidad_mediciones, batch_size, batch_idx, g_rounds, 0, cantidad_series_de_tiempo)\n","                disc_loss_total = train_discriminator(time_series_original_procesada, batch_size, batch_idx, d_rounds, d_rounds, cantidad_series_de_tiempo, cantidad_mediciones)\n","            else:\n","                disc_loss_total = train_discriminator(time_series_original_procesada, batch_size, batch_idx, d_rounds, 0, cantidad_series_de_tiempo, cantidad_mediciones)\n","                g_loss = train_generator(time_series_original_procesada, cantidad_mediciones, batch_size, batch_idx, g_rounds, g_rounds, cantidad_series_de_tiempo)\n","            #Guardamos la ultima perdida del discriminador\n","            d_loss_total.append(disc_loss_total)\n","            #Guardamos la ultima perdida del generador\n","            g_loss_total.append(g_loss)\n","        #Guardamos la ultima loss de los training para luego visualizar la perdidas por epoch\n","        d_loss_total_per_epoch.append(disc_loss_total)\n","        g_loss_total_per_epoch.append(g_loss)\n","        #Cada {sample_interval} Epochs evaluamos el modelo.\n","        if (epoch+1) % sample_interval == 0: #5\n","            #Generamos datos. Originalmente solo creabamos {batch_size}, pero para poder foldearlos necesito la {cantidad_series_de_tiempo}.\n","            z = generate_latent_points(cantidad_series_de_tiempo, cantidad_mediciones, 2)\n","            #mag_fake_generated_data = gan_model.predict(z)\n","            mag_fake_generated_data = g_model.predict(z)\n","            #Ploteamos los datos generados vs el tiempo. Además guardamos una foto.\n","            plot_series_gen(mag_fake_generated_data, diff_samples_tiempo_norm_3dim, batch_size, cantidad_mediciones, epoch + 1, ruta_instancia)\n","            #Foldeamos lo que llevamos del training.\n","            foldeo_training(cantidad_series_de_tiempo, cantidad_mediciones, samplesBoot_tiempo_sin_norm_3dim, mag_fake_generated_data, scaler_mag_normal, ruta_instancia, epoch + 1)\n","            #foldeo_training(cantidad_series_de_tiempo, cantidad_mediciones, samples_tiempo_sin_norm, mag_fake_generated_data, scaler_mag_normal, ruta_instancia, epoch + 1)\n","        #Detenemos el timer del epoch actual.\n","        stop = ptime.time()\n","        time_epoch = stop-start\n","        total_time = total_time + time_epoch\n","        total_system_RAM_usage = 100 - (psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)\n","        print(\"Epoch:\", (epoch + 1), \"-\", int(time_epoch), \"s\\t\", \"d_loss:\", disc_loss_total, '\\tg_loss:', g_loss, '\\tTotal time:', int(total_time/60)\n","        , \"Min.\", '\\tETA:', int(((time_epoch)*(epochs-epoch)/60)), \"Min.\", \"\\tRAM usage:\", total_system_RAM_usage, \"%\")\n","    #Graficamos las perdidas en funcion de las iteraciones (No epochs)\n","    plt.rcParams['figure.figsize'] = [10, 7]\n","    ax = pd.DataFrame({'Generative Loss': g_loss_total,'Discriminative Loss': d_loss_total,}).plot(title='Training loss per iterations', logy=True)\n","    ax.set_xlabel(\"Training iterations\")\n","    ax.set_ylabel(\"Loss\")\n","    plt.savefig(ruta_instancia + '\\\\' + 'Training-loss-per-iterations.png')\n","    plt.show()\n","    #Graficamos las perdidas en funcion de las epochs (No iteraciones)\n","    plt.rcParams['figure.figsize'] = [10, 7]\n","    ax2 = pd.DataFrame({'Generative Loss': g_loss_total_per_epoch,'Discriminative Loss': d_loss_total_per_epoch,}).plot(title='Training loss per Epoch', logy=True)\n","    ax2.set_xlabel(\"Training Epochs\")\n","    ax2.set_ylabel(\"Loss\")\n","    plt.savefig(ruta_instancia + '\\\\' + 'Training-loss-per-Epoch.png')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjwTJKfb3WQa","scrolled":true},"source":["\n","#(TRAINING AND SAVE MODELS) OR ONLY LOADS MODELS\n","if training:\n","    print(\"Start training\")\n","    train(time_series_original_procesada, diff_samples_tiempo_norm_3dim, g_model, gan_model, epochs, batch_size, sample_interval, cantidad_mediciones, cantidad_series_de_tiempo, ruta_instancia, scaler_mag_normal)\n","    print(\"Finished training\")\n","    #GURADAMOS MODELOS DE DISCRIMINADOR, GENERADOR Y GAN\n","    print(\"Tiempo usado para guardar:\", tiempo)\n","    print(\"Models saved in:\")\n","    guarda_modelo(ruta_instancia, d_model, 'discriminador')\n","    guarda_modelo(ruta_instancia, g_model, 'generador')\n","    guarda_modelo(ruta_instancia, gan_model, 'gan')\n","else:\n","    print(\"Loading models from:\", path_models)\n","    #print(path_models)\n","    #'''\n","    #WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","    #https://github.com/keras-team/autokeras/issues/1162\n","    #I did some research and the warning states that the state of the optimizer cant be saved, i.e we could not continue the training from the same point it finished. So its not a problem at all.\n","    #'''\n","    d_model = keras.models.load_model(path_models + \"discriminador.h5\")\n","    g_model = keras.models.load_model(path_models + \"generador.h5\")\n","    gan_model = keras.models.load_model(path_models + \"gan.h5\")\n","    print(\"Models loaded\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMgqLDGM3WQa"},"source":["#MOSTRAMOS LA GRAFICA CON LOS DATOS REALES\n","plot_series_real(samples_magnitud_norm_3dim, diff_samples_tiempo_norm_3dim, ruta_instancia)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3M-7bVju3WQb"},"source":["#GENERAMOS NUEVAS MAGNITUDES Y LOS GRAFICAMOS CON TIEMPO BOOTSTRAPEADO...\n","z = generate_latent_points(cantidad_series_de_tiempo, cantidad_mediciones, 2)\n","generated_mag = g_model.predict(z)\n","#generated_mag = gan_model.predict(z)\n","generated_mag_2dim = generated_mag.reshape((cantidad_series_de_tiempo, cantidad_mediciones))\n","#Ploteamos\n","plot_timeseries(diff_samplesBoot_tiempo_norm_3dim, generated_mag_2dim)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3T7yiC8W3WQb"},"source":["#TRABAJAMOS CON EL MODELO DE LSTM PARA CALCULAR EL SCORE DEL MODELO GAN\n","\n","#Cambiamos la dimension de la magnitud generada\n","generated_mag_3dim = generated_mag_2dim.reshape((cantidad_series_de_tiempo, cantidad_mediciones,1))\n","\n","#Calculamos la suma acumulada para el tiempo real\n","cumsum_diff_samples_tiempo_norm_2dim = np.cumsum(diff_samples_tiempo_norm_3dim[:n_samples_evaluate,:,0], axis=1)\n","#Calculamos la suma acumulada para el tiempo bootstrapeado\n","cumsum_diff_samplesBoot_tiempo_norm_2dim = np.cumsum(diff_samplesBoot_tiempo_norm_3dim[:n_samples_evaluate,:,0], axis=1)\n","\n","#Juntamos el tiempo y magnitud reales ya procesados\n","time_series_original_procesada = np.vstack(((np.reshape(cumsum_diff_samples_tiempo_norm_2dim, (n_samples_evaluate, cantidad_mediciones, 1))).T, samples_magnitud_norm_3dim[:n_samples_evaluate].T)).T\n","#Juntamos el tiempo bootstrapeado y la magnitud generada\n","time_series_boots_procesada = np.vstack(((np.reshape(cumsum_diff_samplesBoot_tiempo_norm_2dim, (n_samples_evaluate, cantidad_mediciones, 1))).T, generated_mag_3dim[:n_samples_evaluate,:,:].T)).T\n","\n","#Juntamos las series de tiempo reales con las falsas\n","time_series_original_y_boots = np.concatenate((time_series_original_procesada, time_series_boots_procesada), axis=0)\n","\n","#Creamos vectores binarios\n","vector_binario_1 = np.ones_like(time_series_original_procesada) #REAL\n","vector_binario_0 = np.zeros_like(time_series_boots_procesada) #FALSO\n","#Juntamos los vectores binarios\n","vector_binario = np.concatenate((vector_binario_1, vector_binario_0), axis=0) #Real/Falsa\n","\n","#INFO\n","print(\"cumsum_diff_samples_tiempo_norm_2dim:\", cumsum_diff_samples_tiempo_norm_2dim.shape)\n","print(\"cumsum_diff_samplesBoot_tiempo_norm_2dim:\", cumsum_diff_samplesBoot_tiempo_norm_2dim.shape)\n","print(\"time_series_original_procesada:\", time_series_original_procesada.shape)\n","print(\"time_series_boots_procesada:\", time_series_boots_procesada.shape)\n","print(\"time_series_original_y_boots:\", time_series_original_y_boots.shape)\n","print(\"vector_binario:\", vector_binario.shape)\n","\n","#Obtenemos los datos de entrenamiento y evaluacion\n","x_train, x_test, y_train, y_test = train_test_split(time_series_original_y_boots, vector_binario, test_size=0.2, random_state=5)\n","#De primer parametro se le pasa la serie de tiempo original concatenada con la falsa. De segundo parametro recibe los vectores binarios 0 y 1.\n","#x_train son datos de entrenamiento\n","#x_test son datos para testear\n","#y_train e y_test son los vectores unitarios (1 y 0) para los \"test_size\" de datos que se esten utilizando\n","\n","#INFO\n","print(\"x_train:\", x_train.shape)\n","print(\"y_train:\", y_train.shape)\n","print(\"x_test:\", x_test.shape)\n","print(\"y_test\", y_test.shape)\n","print(\"len_vec_lstm_score:\", len_vec_lstm_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ob1re4lX3WQb","scrolled":true},"source":["#(TRAINING AND SAVE MODEL) OR ONLY LOAD MODEL\n","if training:\n","    print(\"Training\")\n","    history = lstm_score_model.fit(x_train, y_train, epochs=epochs_score, validation_data=(x_test, y_test))\n","    print(\"Training finished\")\n","    print(\"Tiempo usado para guardar:\", tiempo)\n","    print(\"Saving model\")\n","    guarda_modelo(ruta_instancia, lstm_score_model, 'lstm_score')\n","    print(\"Model saved\")\n","else:\n","    print(\"Loading models\")\n","    lstm_score_model = keras.models.load_model(path_models + \"lstm_score.h5\")\n","    print(\"Models loaded\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h01Ul5lz3WQb"},"source":["#Evaluamos la LSTM con los datos de testing\n","results = lstm_score_model.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JPwG1pb3WQc"},"source":["#PLOTEAMOS EL ACCURACY DE LA LSTM\n","if training:\n","    plt.plot(history.history['loss'], c='r', label='Loss')\n","    plt.plot(history.history['accuracy'], c='b', label='Accuracy')\n","    #plt.plot(xlabel='Epochs')\n","    plt.legend()\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Percentage (%)')\n","    plt.title('LSTM Score of GAN')\n","    plt.savefig(ruta_instancia + '\\\\' + 'LSTM-Score.png')\n","    plt.show()\n","    ## Final evaluation of the model\n","    scores = lstm_score_model.evaluate(x_test, y_test, batch_size=n_input, verbose=0)\n","    print(scores)\n","    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","    print(\"val_accuracy: %.2f%%\" % (scores[0]*100))\n","    lista_resultados.append(scores[1]*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3FZUBGp3WQc"},"source":["#ZONA DE FOLDEO\n","#PRIMERO FOLDEAMOS LAS SERIES DE TIEMPO GENERADAS Y LUEGO LAS REALES"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLlH92n-3WQc"},"source":["#PROCESAMOS LOS DATOS GENERADOS PARA FOLDEARLOS\n","\n","#Desnormalizamos la magnitud generada\n","generated_mag_sin_norm_2dim = scaler_mag_normal.inverse_transform(generated_mag_2dim.T).T\n","#Cambiamos la dimension\n","generated_mag_sin_norm_3dim = generated_mag_sin_norm_2dim.reshape((cantidad_series_de_tiempo, cantidad_mediciones, 1))\n","#Juntamos el tiempo boostrapeado con la magnitud generada\n","time_series_generada_3dim = np.vstack((samplesBoot_tiempo_sin_norm_3dim.T, generated_mag_sin_norm_3dim.T)).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ik4qCM4O3WQc"},"source":["#Nombre de columnas de la instnacia del archivo de OGLE\n","nombre_columnas = ['time', 'magnitud', 'error'] \n","#Rellenamos la columna 'error' de la libreria para foldear (no podemos foldear sin esta columna).\n","test_0 = np.zeros((cantidad_series_de_tiempo, cantidad_mediciones, 1))\n","#Concatenamos la columna de 0's\n","time_series_generada_3dim = np.vstack((time_series_generada_3dim.T, test_0.T)).T\n","#Pasamos la serie de tiempo al df y utilizamos solo 1 serie de tiempo para ver el foldeo\n","df_generado = pd.DataFrame(time_series_generada_3dim[20,:,:], columns=nombre_columnas)\n","if 'WISE' not in ruta_instancia:\n","    #Sumamos un escalar al tiempo para un foldeo correcto\n","    df_generado['time'] = df_generado['time'] + 2450000\n","df_generado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpGHs-mI1JBG"},"source":["df_generado_dtype_float = df_generado.astype(float) #Creamos una copia del {df_generado} como flotante.\n","#print(df_generado_dtype_float)\n","\n","if training:\n","    df_generado_dtype_float.to_csv(path_or_buf=ruta_instancia + '\\\\' + 'SerieTiempo-Generada.csv', sep=' ', index=False)\n","\n","#list(Time.FORMATS)\n","\n","#Convertimos el df a un LightCurve\n","lc_generado = lk.LightCurve(time=df_generado_dtype_float['time'], flux=df_generado_dtype_float['magnitud'], flux_err=df_generado_dtype_float['error'])\n","lc_generado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOlKK0tk3WQc"},"source":["#Calculamos el periodograma usando el metodo de'lombscargle'\n","pg_generado = lc_generado.to_periodogram(method='lombscargle', oversample_factor=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wb3YBeMX3WQc"},"source":["#PLOTEAMOS EL PERIODOGRAMA\n","periodograma = pg_generado.plot()\n","periodograma.set_title(\"Periodograma generado\")\n","if training:\n","    periodograma.figure.savefig(ruta_instancia + '\\\\' + 'Periodograma-generado.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YKOVcpc3WQd"},"source":["#INFO\n","pg_generado.show_properties()\n","print(\"Frequency at max power:\", pg_generado.frequency_at_max_power)\n","print(\"Periodo:\", pg_generado.period_at_max_power)\n","lista_resultados.append(pg_generado.period_at_max_power)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-L98x5Nw3WQd"},"source":["#PLOTEAMOS LA SERIE DE TIEMPO EN BRUTO\n","#plt.plot(df_generado_dtype_float['time'], df_generado_dtype_float['magnitud'], 'o', markersize=1)\n","#plt.xlabel('Julian Date')\n","#plt.ylabel('Magnitud')\n","#plt.title('Serie de tiempo generada')\n","#plt.savefig(ruta_instancia + '\\\\' + 'SerieTiempo-Generada.png')\n","#plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-PntnXg3WQd"},"source":["#FOLDEO\n","#Todavia no puedo foldear con la fase normalizada en la misma funcion ya que elimina la unidad de 'JD' y no he logrado solucionarlo. Usar dias como unidad tampoco funciona.\n","lc_generado_foldeado = lc_generado.fold(pg_generado.period_at_max_power, normalize_phase=False)\n","print(lc_generado_foldeado)\n","lc_scatter = lc_generado_foldeado.scatter(color='red')\n","lc_scatter.set_title(\"Serie de tiempo generada\")\n","if training:\n","    lc_scatter.figure.savefig(ruta_instancia + '\\\\' + 'SerieTiempo-Generada-LightCurve-Scatter.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkSF5jY-3WQd"},"source":["#Normalizamos las magnitudes\n","lc_generado_normalizado = lc_generado.normalize()\n","lc_generado_normalizado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAvX2a7o3WQd"},"source":["#Intento de samplear y normalizar el foldeo\n","median_generada = st.median(lc_generado_foldeado['flux'])\n","mean_generada = st.mean(lc_generado_foldeado['flux'].value)\n","stddev_generada = st.stdev(lc_generado_foldeado['flux'].value)\n","\n","lc_generado_foldeado['flux_norm'] = lc_generado_foldeado['flux'] / mean_generada"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaN8xkso3WQd"},"source":["lc_generado_foldeado_binned = lc_generado_foldeado.bin(0.08)\n","lc_generado_foldeado_binned"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTyxxY8n3WQd"},"source":["plt.plot(lc_generado_foldeado.time.jd, lc_generado_foldeado['flux_norm'], 'k.', markersize=1)\n","plt.plot(lc_generado_foldeado_binned.time_bin_start.jd, lc_generado_foldeado_binned['flux_norm'], 'r-', drawstyle='steps-mid')\n","plt.xlabel('Time (days)')\n","plt.ylabel('Normalized flux')\n","plt.title(\"Foldeo - Generado\")\n","if training:\n","    plt.savefig(ruta_instancia + '\\\\' + 'Foldeo-Generado.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evwIbqbr3WQd"},"source":["#PROCESAMOS LOS DATOS REALES PARA FOLDEARLOS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LInCQ-N-wB2G"},"source":["#Seleccionamos una serie de tiempo real al azar para foldear\n","x = random.randint(0,len(lista_nombre_instancias)-1)\n","nombre_serie_tiempo = lista_nombre_instancias[x]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KQmLpnm3WQd"},"source":["#Ruta de la serie de tiempo a foldear\n","path_photometry = path_datos + '\\\\' + nombre_serie_tiempo + extension_series_de_tiempo\n","print(\"Cargando archivo:\", path_photometry)\n","\n","#Cargamos la serie de tiempo\n","if 'OGLE' in path_index: \n","    df_real = pd.read_csv(path_photometry, sep=' ', names=['time', 'magnitud', 'error', '1', '2', '3', '4'])\n","elif 'GAIA' in path_index:\n","    df_real = pd.read_csv(path_photometry, sep=',', names=['time', 'magnitud', 'error', '1', '2', '3', '4'], skiprows=1)\n","elif 'WISE' in path_index:\n","    df_real = pd.read_csv(path_photometry, sep=',', names=['time', 'magnitud', 'error', '1', '2', '3', '4'], skiprows=1)\n","else:\n","    print(\"ERROR #### NO SE PUDO CARGAR LA SERIE DE TIEMPO ####\")\n","\n","#Como hay datos que vienen con muchas columnas, eliminamos las columnas extras que tengan\n","while len(df_real.columns) > 3:\n","    df_real.drop(df_real.columns[len(df_real.columns)-1], axis=1, inplace=True)\n","\n","#Modificamos el tiempo para que podamos foldear la serie de tiempo\n","if 'WISE' not in ruta_instancia:\n","    df_real['time'] = df_real['time'] + 2450000\n","#df_real = df_real.loc[:1299]\n","df_real"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0mNx4ZF3WQe"},"source":["lc_real = lk.LightCurve(time=df_real['time'], flux=df_real['magnitud'], flux_err=df_real['error'])\n","lc_real"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvp_wsW-3WQe"},"source":["pg_real = lc_real.to_periodogram(method='lombscargle', oversample_factor=10) #Calculamos el periodograma usando el metodo de'lombscargle'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFk_elXT3WQe"},"source":["periodograma = pg_real.plot()\n","periodograma.set_title(\"Periodograma real\")\n","if training:\n","    periodograma.figure.savefig(ruta_instancia + '\\\\' + 'Periodograma-real.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Faq3C2VC3WQe"},"source":["pg_real.show_properties()\n","print(\"Frequency at max power:\", pg_real.frequency_at_max_power)\n","print(\"Periodo:\", pg_real.period_at_max_power)\n","lista_resultados.append(pg_real.period_at_max_power)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HqTZVlpL3WQe"},"source":["plt.plot(df_real['time'], df_real['magnitud'], 'o', markersize=1)\n","plt.xlabel('Julian Date')\n","plt.ylabel('Magnitud')\n","plt.title(nombre_serie_tiempo)\n","if training:\n","    plt.savefig(ruta_instancia + '\\\\' + 'SerieTiempo-Original-' + nombre_serie_tiempo + '.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLcL-6yF3WQe"},"source":["#Todavia no puedo foldear con la fase normalizada en la misma funcion ya que elimina la unidad de 'JD' y no he logrado solucionarlo. Usar dias como unidad tampoco funciona.\n","lc_real_foldeada = lc_real.fold(pg_real.period_at_max_power, normalize_phase=False)\n","print(lc_real_foldeada)\n","lc_scatter = lc_real_foldeada.scatter(color='red')\n","lc_scatter.set_title(\"Serie de tiempo real - Binned\")\n","#lc_scatter.figure.savefig(ruta_instancia + '\\\\' + 'Binned-SerieTiempo-Original-LightCurve-Scatter-' + nombre_serie_tiempo + '.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-UhbBLl3WQe"},"source":["lc_real_normalizada = lc_real.normalize()\n","lc_real_normalizada"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMhVe8qN3WQe"},"source":["median_real = st.median(lc_real_foldeada['flux'])\n","mean_real = st.mean(lc_real_foldeada['flux'].value)\n","stddev_real = st.stdev(lc_real_foldeada['flux'].value)\n","\n","lc_real_foldeada['flux_norm'] = lc_real_foldeada['flux'] / mean_real"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTqPM6FC3WQe"},"source":["lc_real_foldeada_binned = lc_real_foldeada.bin(0.08)\n","lc_real_foldeada_binned"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqqFzn1c3WQe"},"source":["plt.plot(lc_real_foldeada.time.jd, lc_real_foldeada['flux_norm'], 'k.')\n","plt.plot(lc_real_foldeada_binned.time_bin_start.jd, lc_real_foldeada_binned['flux_norm'], 'r-', drawstyle='steps-post')\n","plt.xlabel('Time (days)')\n","plt.ylabel('Normalized flux')\n","plt.title('Folded ' + nombre_serie_tiempo)\n","if training:\n","    plt.savefig(ruta_instancia + '\\\\' + 'Foldeo-Original-' + nombre_serie_tiempo + '.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Az51wEMNmfGY"},"source":["#Guardamos algunos resultados del trabajo\n","if training:\n","    guarda_resultados(config, path_models_root, tiempo, lista_resultados)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F0Hz0HazMELN"},"source":["#Probar foldear con el periodo real.......................................TODO"],"execution_count":null,"outputs":[]}]}